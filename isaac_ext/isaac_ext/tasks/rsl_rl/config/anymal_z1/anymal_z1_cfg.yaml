num_steps_per_env: 24
max_iterations: 8000
save_interval: 50
experiment_name: anymal_d_z1_flat
empirical_normalization: False
logger: wandb
wandb_project: reacher_rl
device: cuda
seed: 42
num_envs: 4096
resume: False
policy:
  class_name: ActorCritic
  init_noise_std: 1.0
  actor_hidden_dims: [512, 256, 128]
  critic_hidden_dims: [512, 256, 128]
  activation: "elu"
algorithm:
  class_name: PPO
  value_loss_coef: 1.0
  use_clipped_value_loss: True
  clip_param: 0.2
  entropy_coef: 0.005
  num_learning_epochs: 5
  num_mini_batches: 4
  learning_rate: 0.001
  schedule: adaptive
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.01
  max_grad_norm: 1.0

hydra:
  run:
    dir: logs/reacher_rl/${now:%b-%d}/${now:%H-%M-%S}
